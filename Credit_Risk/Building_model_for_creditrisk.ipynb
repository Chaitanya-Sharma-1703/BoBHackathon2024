{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":392639,"sourceType":"datasetVersion","datasetId":173764},{"sourceId":6453754,"sourceType":"datasetVersion","datasetId":3725579}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loan Default Prediction","metadata":{}},{"cell_type":"markdown","source":"> ****Financial institutions, including banks, credit unions, and government lenders, provide loan services to individuals and businesses.\nA major concern for these institutions is loan defaults, where borrowers fail to repay their loans as agreed.\nTo manage this risk, many institutions utilize machine learning algorithms to predict which borrowers are most likely to default.****\n\n> The Dataset:\nThis specific dataset originates from Coursera's Loan Default Prediction Challenge.\nIt offers an opportunity to practice and test machine learning skills on a real-world problem.\nThe dataset comprises a significant amount of data, with 255,347 rows and 18 columns.\nPresumably, these columns contain information relevant to borrowers and their loan characteristics.\n\n> The Goal:\nThe primary objective is to utilize the data to build a model that accurately predicts loan defaults.\nThis would enable financial institutions to:\nIdentify high-risk borrowers at an early stage.\nImplement targeted interventions, such as loan restructuring or financial counseling, to prevent defaults.\nMake more informed lending decisions, potentially reducing overall risk exposure.","metadata":{}},{"cell_type":"code","source":"# load the data \nimport numpy as np\nimport pandas as pd\nloan=pd.read_csv(\"Loan_default.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#info\nloan.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label encoding\nfrom sklearn.preprocessing import LabelEncoder\n# Assuming df is your DataFrame\ncategorical_columns = ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n\n# Initializing LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encoding categorical columns\nfor col in categorical_columns:\n    loan[col] = label_encoder.fit_transform(loan[col])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EDA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncolumns = loan.columns\nfig, axs = plt.subplots(5, 2, figsize=(10, 10))\nsns.boxplot(loan.iloc[:,1],ax=axs[0, 0],orient='h').set_title(columns[1])\nsns.boxplot(loan.iloc[:,2],ax=axs[0, 1],orient='h').set_title(columns[2])\nsns.boxplot(loan.iloc[:,3],ax=axs[1, 0],orient='h').set_title(columns[3])\nsns.boxplot(loan.iloc[:,4],ax=axs[1, 1],orient='h').set_title(columns[4])\nsns.boxplot(loan.iloc[:,5],ax=axs[2, 0],orient='h').set_title(columns[5])\nsns.boxplot(loan.iloc[:,6],ax=axs[2, 1],orient='h').set_title(columns[6])\nsns.boxplot(loan.iloc[:,7],ax=axs[3, 0],orient='h').set_title(columns[7])\nsns.boxplot(loan.iloc[:,8],ax=axs[3, 1],orient='h').set_title(columns[8])\nsns.boxplot(loan.iloc[:,9],ax=axs[4, 0],orient='h').set_title(columns[9])\nfig.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if our observations are correct\nnp.round(loan.pivot_table(index='Default',values=['InterestRate','LoanAmount','NumCreditLines'],aggfunc=('mean','std')),2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if our observations are correct\nnp.round(loan.pivot_table(index='Default',values=['Age','Income','MonthsEmployed'],aggfunc=('mean','std')),2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loan.isnull()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split Dataset into train and test dataset\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(loan.drop(columns=['LoanID','Default']),loan['Default'], test_size=0.25,random_state=42)\n\nprint('x_train:',x_train.shape)\nprint('y_train:',y_train.shape)\nprint('x_test:',x_test.shape)\nprint('y_test:',y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ML models - Ensemble methods\n\n> #### Ensemble methods\n     Bagging\n     Boosting\n     Stacking\n     Voting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import  BaggingClassifier, GradientBoostingClassifier,RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bagging Method","metadata":{}},{"cell_type":"code","source":"# Create a base classifier (e.g., DecisionTreeClassifier):\nbase_clf = DecisionTreeClassifier()\n\n# Create the bagging classifier:\nbagging_clf = BaggingClassifier(base_estimator=base_clf, n_estimators=100, random_state=42)\n\n# Train and evaluate (similar to Voting):\nbagging_clf.fit(x_train, y_train)\ny_pred = bagging_clf.predict(x_test)\nbagging_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Bagging accuracy:\", bagging_accuracy)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boosting method","metadata":{}},{"cell_type":"code","source":"gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\ngb_clf.fit(x_train, y_train)\ny_pred = gb_clf.predict(x_test)\nboosting_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Boosting accuracy:\", boosting_accuracy)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### stacking Method","metadata":{}},{"cell_type":"code","source":"# Define the base models\nbase_models = [\n    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42))\n]\n\n# Define the meta-model\nmeta_model = LogisticRegression()\n\n# Create the stacking classifier\nstacking_classifier = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n\n# Train the stacking classifier on the training data\nstacking_classifier.fit(x_train, y_train)\n\n# Make predictions on the test set\nstacking_predictions = stacking_classifier.predict(x_test)\n\n# Evaluate the performance\nstack_accuracy = accuracy_score(y_test, stacking_predictions)\nprint(f'Accuracy of Stacking Classifier: {stack_accuracy}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tree based Learning Methods","metadata":{}},{"cell_type":"markdown","source":"### Random Forest:","metadata":{}},{"cell_type":"code","source":"\n\nrf_clf = RandomForestClassifier(n_estimators=100,  random_state=42)\n\n# Train and evaluate:\nrf_clf.fit(x_train, y_train)\ny_pred = rf_clf.predict(x_test)\nrandom_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Random Forest accuracy:\", random_accuracy)\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Decision Tree:","metadata":{}},{"cell_type":"code","source":"\ndt_clf = DecisionTreeClassifier(random_state=42)\n\n# Train and evaluate:\ndt_clf.fit(x_train, y_train)\ny_pred = dt_clf.predict(x_test)\ndt_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Decision Tree accuracy:\", dt_accuracy)\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy scores for different models\naccuracies = {\n    \"Decision Tree\": dt_accuracy,\n    \"Random Forest\": random_accuracy,\n    \"Stacking\": stack_accuracy,\n    \"Boosting\": boosting_accuracy,\n    \"Bagging\": bagging_accuracy\n}\n\n\naccuracies = dict(sorted(accuracies.items(), key=lambda item: item[1], reverse=True))\n\n# Create the bar chart\nplt.figure(figsize=(16, 9))  # Adjust figure size as needed\n\ncolors = ['lightblue' if model != 'Boosting' else 'orange' for model in accuracies.keys()]\nplt.bar(accuracies.keys(), accuracies.values(), color=colors)\n# Customize chart elements\nplt.xlabel(\"Model Type\", fontsize=14)\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.title(\"Accuracy Comparison of Different Models\", fontsize=18)\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\nplt.ylim(0, 1.1)  # Set y-axis limits for precision\n\nplt.grid(axis='y', linestyle='--', alpha=0.8)  # Add grid lines\n\n# Annotate each bar with 8 decimal places\nfor i, v in enumerate(accuracies.values()):\n    plt.text(i, v + 0.005, f\"{v:.4f}\")  # Format with 8 decimals\n\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## voting classifier","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.ensemble import VotingClassifier\n\n# Create a list of tuples with (model_name, model_instance)\nmodels = [ ( \"Decision Tree\", dt_clf), (\"Random Forest\",rf_clf), ( \"Stacking\", stacking_classifier), \n          (\"Boosting\", gb_clf),(\"Bagging\", bagging_clf)]\n\n# Create a Voting Classifier\nvoting_classifier = VotingClassifier(estimators=models, voting='hard')  # 'hard' for majority voting\n\n\n# Fit the Voting Classifier on the training data\nvoting_classifier.fit(x_train, y_train)\n\n# Predictions\npredictions = voting_classifier.predict(x_test)\n\n# Calculate and print accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f'Ensemble Model Accuracy: {accuracy}')\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the bar chart\n# Calculate and print accuracy\naccuracy_ensemble = accuracy_score(y_test, predictions)\nprint(f'Ensemble Model Accuracy: {accuracy}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy scores for different models\naccuracies = {\n    \"Decision Tree\": dt_accuracy,\n    \"Random Forest\": random_accuracy,\n    \"Stacking\": stack_accuracy,\n    \"Boosting\": boosting_accuracy,\n    \"Bagging\": bagging_accuracy,\n    \"Voting\":accuracy_ensemble  # Add accuracy for the ensemble model\n}\n\n# Sort the accuracies dictionary by values in descending order\naccuracies = dict(sorted(accuracies.items(), key=lambda item: item[1], reverse=True))\n\n# Create the bar chart\nplt.figure(figsize=(16, 9))  # Adjust figure size as needed\n\n# Define colors for each bar\ncolors = ['lightblue' if model != 'Voting' else 'red' for model in accuracies.keys()]\n\n# Plot the bars\nplt.bar(accuracies.keys(), accuracies.values(), color=colors)\n\n# Customize chart elements\nplt.xlabel(\"Model Type\", fontsize=14)\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.title(\"Accuracy Comparison of Different Models\", fontsize=18)\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\nplt.ylim(0, 1.1)  # Set y-axis limits for precision\n\nplt.grid(axis='y', linestyle='--', alpha=0.8)  # Add grid lines\n\n# Annotate each bar with 8 decimal places\nfor i, v in enumerate(accuracies.values()):\n    plt.text(i, v + 0.005, f\"{v:.4f}\")  # Format with 8 decimals\n\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}